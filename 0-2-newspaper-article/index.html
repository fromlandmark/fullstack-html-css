<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta http-equiv="X-UA-Compatible" content="IE=edge" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<title>Document</title>
	</head>
	<style>
		html {
			font-family: Georgia, "Times New Roman", Times, serif;
			line-height: 150%;
		}
	</style>
	<body>
		<article>
			<img src="./assets/thumbnail.webp" alt="" />
			<p>
				Researchers are starting to unravel one of the biggest mysteries behind the AI language models that power text
				and image generation tools like DALL-E and ChatGPT.
			</p>
			<p>
				For a while now, machine learning experts and scientists have noticed something strange about large language
				models (LLMs) like OpenAI’s GPT-3 and Google’s LaMDA: they are inexplicably good at carrying out tasks that they
				haven’t been specifically trained to perform. It’s a perplexing question, and just one example of how it can be
				difficult, if not impossible in most cases, to explain how an AI model arrives at its outputs in fine-grained
				detail.
			</p>
			<p>
				In a forthcoming study posted to the arXiv preprint server, researchers at the Massachusetts Institute of
				Technology, Stanford University, and Google explore this “apparently mysterious” phenomenon, which is called
				“in-context learning.” Normally, to accomplish a new task, most machine learning models need to be retrained on
				new data, a process that can normally require researchers to input thousands of data points to get the output
				they desire—a tedious and time-consuming endeavor.
			</p>
			<p>
				But with in-context learning, the system can learn to reliably perform new tasks from only a few examples,
				essentially picking up new skills on the fly. Once given a prompt, a language model can take a list of inputs
				and outputs and create new, often correct predictions about a task it hasn’t been explicitly trained for. This
				kind of behavior bodes very well for machine learning research, and unraveling how and why it occurs could yield
				invaluable insights into how language models learn and store information.
			</p>
			<p>But what’s the difference in a model that learns, and doesn’t merely memorize?</p>
			<summary>
				<span>Machine Learning</span>
				<span>Machine</span>
				<span>Web</span>
				<span>Tech</span>
			</summary>
		</article>
	</body>
</html>
